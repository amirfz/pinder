{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import getpass\n",
    "import base64\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, log_loss, f1_score\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connecting to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient('localhost:27017')\n",
    "db = client.arXivDB\n",
    "db.users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(doc, stem=False):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    doc = doc.lower() # turn text to lowercase\n",
    "\n",
    "    stops = set(stopwords.words(\"english\"))       # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                   # Creating the stemmer model\n",
    "\n",
    "    doc = re.sub(r\"quantum\", '', doc)           # removing the word quantum (duh)\n",
    "    doc = re.sub(r\"physics\", '', doc)           # removing the word physics (duh)\n",
    "    doc = re.sub(r\"state\", '', doc)           # removing the word state (duh)\n",
    "    doc = re.sub(r'\\$.*?\\$', 'latexinlineformula', doc) # replacing latex inline formula\n",
    "    doc = re.sub(r'\\\\n', ' ', doc) # removing new line character\n",
    "    doc = re.sub(r'\\\\\\\\\\\"', '', doc)             # removing german double dotted letters\n",
    "    doc = re.sub(r\"</?\\w+[^>]*>\", '', doc)      # removing html tags\n",
    "    doc = re.sub(\"[^a-zA-Z]\", ' ', doc)    # removing anythin other alpha-numerical char's and @ and !\n",
    "\n",
    "    doc = doc.split()                          # Splits the data into individual words \n",
    "    doc = [w for w in doc if not w in stops and len(w) > 3]   # Removes stopwords and short length words\n",
    "    if stem:\n",
    "        doc = [p_stemmer.stem(i) for i in doc]     # Stemming (reducing words to their root)\n",
    "    if not len(doc):                            # dealing with comments that are all emojis, stop words or other languages\n",
    "        doc = ['emptystring']\n",
    "    # print('text cleaning done!')\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class feature_stacker(BaseEstimator):\n",
    "    \"\"\"Stacks several transformer objects to yield concatenated features.\n",
    "    Similar to pipeline, a list of tuples ``(name, estimator)`` is passed\n",
    "    to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_list):\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        feature_names = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            feature_names.extend(trans.get_feature_names())\n",
    "        feature_names = [\" \".join(w) if isinstance(w, tuple) else w\n",
    "                            for w in feature_names]\n",
    "        return np.array(feature_names)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for name, trans in self.transformer_list:\n",
    "            trans.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            features.append(trans.transform(X))\n",
    "        issparse = [sparse.issparse(f) for f in features]\n",
    "        if np.any(issparse):\n",
    "            features = sparse.hstack(features).tocsr()\n",
    "        else:\n",
    "            features = np.hstack(features)\n",
    "        return features\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        if not deep:\n",
    "            return super(feature_stacker, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = dict(self.transformer_list)\n",
    "            for name, trans in self.transformer_list:\n",
    "                for key, value in trans.get_params(deep=True).items():\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(lowercase=False,\n",
    "                                 analyzer=u'word',\n",
    "                                 ngram_range=(1, 3),\n",
    "                                 stop_words='english',\n",
    "                                 binary=False,\n",
    "                                 norm=u'l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=True, \n",
    "                                 sublinear_tf=True,\n",
    "                                 min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_char = TfidfVectorizer(lowercase=False,\n",
    "                                 analyzer=u'char',\n",
    "                                 ngram_range=(1, 5),\n",
    "                                 stop_words='english',\n",
    "                                 binary=False,\n",
    "                                 norm=u'l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=True, \n",
    "                                 sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft = feature_stacker([(\"chars\", vectorizer_char),\n",
    "                      (\"words\", vectorizer_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select = SelectPercentile(score_func=chi2, percentile=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: amir\n",
      "pin: ········\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    username = input('username: ').lower()\n",
    "    user = list(db.users.find({'username': username}))\n",
    "    if not user:\n",
    "        print('the username doesnt exist, try again')\n",
    "        user = None\n",
    "    else:\n",
    "        pin = base64.b64encode(bytes(str(getpass.getpass('pin: ')), encoding=\"UTF-8\"))\n",
    "        if not user[0]['pin']==pin:\n",
    "            print('pin is incorrect, try again')\n",
    "            user = None\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_results = list(db.likes.find({'user_id':user[0]['_id']}, {'paper_id':1, '_id':0, 'like':1}))\n",
    "mypaper_ids = [d['paper_id'] for d in query_results]\n",
    "mylikes = [d['like'] for d in query_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [cleaner(' '.join([d['title'], d['summary']])) for d in db.arXivfeeds.find(\n",
    "                    {'_id': {'$in': mypaper_ids}}, {'_id':0, 'title':1, 'summary':1}\n",
    "                )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(mylikes)\n",
    "Y = le.transform(mylikes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft.fit(documents)\n",
    "X = ft.transform(documents)\n",
    "select.fit(X, Y)\n",
    "X = select.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.623006\teval-logloss:0.627719\n",
      "[1]\ttrain-logloss:0.565238\teval-logloss:0.57061\n",
      "[2]\ttrain-logloss:0.517152\teval-logloss:0.524949\n",
      "[3]\ttrain-logloss:0.476384\teval-logloss:0.487714\n",
      "[4]\ttrain-logloss:0.441729\teval-logloss:0.45434\n",
      "[5]\ttrain-logloss:0.412093\teval-logloss:0.426585\n",
      "[6]\ttrain-logloss:0.38653\teval-logloss:0.404615\n",
      "[7]\ttrain-logloss:0.364114\teval-logloss:0.385751\n",
      "[8]\ttrain-logloss:0.344606\teval-logloss:0.367945\n",
      "[9]\ttrain-logloss:0.327854\teval-logloss:0.353738\n",
      "[10]\ttrain-logloss:0.312915\teval-logloss:0.342274\n",
      "[11]\ttrain-logloss:0.300109\teval-logloss:0.330948\n",
      "[12]\ttrain-logloss:0.288673\teval-logloss:0.322787\n",
      "[13]\ttrain-logloss:0.278726\teval-logloss:0.31566\n",
      "[14]\ttrain-logloss:0.270138\teval-logloss:0.309224\n",
      "[15]\ttrain-logloss:0.262358\teval-logloss:0.304291\n",
      "[16]\ttrain-logloss:0.255279\teval-logloss:0.300601\n",
      "[17]\ttrain-logloss:0.249374\teval-logloss:0.296279\n",
      "[18]\ttrain-logloss:0.244292\teval-logloss:0.29346\n",
      "[19]\ttrain-logloss:0.23957\teval-logloss:0.291392\n",
      "[20]\ttrain-logloss:0.235674\teval-logloss:0.288883\n",
      "[21]\ttrain-logloss:0.232174\teval-logloss:0.286982\n",
      "[22]\ttrain-logloss:0.229195\teval-logloss:0.285837\n",
      "[23]\ttrain-logloss:0.226189\teval-logloss:0.285262\n",
      "[24]\ttrain-logloss:0.223652\teval-logloss:0.284819\n",
      "[25]\ttrain-logloss:0.221742\teval-logloss:0.284521\n",
      "[26]\ttrain-logloss:0.219677\teval-logloss:0.284545\n",
      "[27]\ttrain-logloss:0.217973\teval-logloss:0.284624\n",
      "[28]\ttrain-logloss:0.216389\teval-logloss:0.284901\n",
      "[29]\ttrain-logloss:0.215098\teval-logloss:0.285219\n",
      "[30]\ttrain-logloss:0.214173\teval-logloss:0.285553\n",
      "[31]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[32]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[33]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[34]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[35]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[36]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[37]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[38]\ttrain-logloss:0.213076\teval-logloss:0.28603\n",
      "[39]\ttrain-logloss:0.213076\teval-logloss:0.28603\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#     XGBoost\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"eta\": 0.1,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [122, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-d3e21565718d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'confusion matrix:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [122, 10]"
     ]
    }
   ],
   "source": [
    "pred = gbm.predict(xgb.DMatrix(X_test))\n",
    "print('confusion matrix:')\n",
    "print(confusion_matrix(y_test, pred>0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_papers_toshow = 10\n",
    "test_documents = ['___'.join([d['title'], d['summary']]) for d in db.arXivfeeds.aggregate([\n",
    "            {\"$sample\": {'size': num_papers_toshow}}\n",
    "        ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = ft.transform([cleaner(d) for d in test_documents])\n",
    "X_test = select.transform(X_test)\n",
    "pred = gbm.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_documents = [x for (y,x) in sorted(zip(pred, test_documents))]\n",
    "pred = np.sort(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0603464\n",
      "Distinguishability of apparatus states in quantum measurement in the\n",
      "  Stern-Gerlach experiment\n",
      "0.0603464\n",
      "Optimal control of time-dependent targets\n",
      "0.0603464\n",
      "Peak Doubling in SPDC Coincidence Spectra with a Short-Pulse Pump\n",
      "0.0603464\n",
      "The Born rule from a consistency requirement on hidden measurements in\n",
      "  complex Hilbert space\n",
      "0.0603464\n",
      "Tomographic approach to the violation of Bell's inequalities for quantum\n",
      "  states of two qutrits\n",
      "0.0622891\n",
      "Radiative Corrections to Multi-Level Mollow-Type Spectra\n",
      "0.0686694\n",
      "Dimensional Crossover in Bragg Scattering from an Optical Lattice\n",
      "0.104405\n",
      "Optimal arbitrarily accurate composite pulse sequences\n",
      "0.159957\n",
      "Optimal two-qubit gate for generation of random bipartite entanglement\n",
      "0.232279\n",
      "Entanglement and nonclassicality in four-mode Gaussian states generated\n",
      "  via parametric down-conversion and frequency up-conversion\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(pred)):\n",
    "    print(pred[j])\n",
    "    print(test_documents[j].split('___')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
